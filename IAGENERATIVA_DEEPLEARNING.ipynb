{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2lGBTKRGE5X"
   },
   "source": [
    "# Generaci√≥n de contenido con IA generativa  \n",
    "### Deep Learning Avanzado ‚Äì IU Digital  \n",
    "### Docente: **Laura Alejandra S√°nchez**  \n",
    "### Grupo: **PREICA2502B020125**\n",
    "\n",
    "---\n",
    "\n",
    "## Integrantes del equipo\n",
    "- **Juliana Mar√≠a Pe√±a Su√°rez**  \n",
    "- **Juan Esteban Atehort√∫a S√°nchez**  \n",
    "- **Nikol Tamayo R√∫a**\n",
    "\n",
    "---\n",
    "\n",
    "## Proyecto: Generaci√≥n de contenido visual con modelos de IA generativa\n",
    "\n",
    "Este proyecto corresponde a la Evidencia de Aprendizaje de la Unidad 3 del curso **Deep Learning Avanzado**, y tiene como objetivo implementar un modelo de Inteligencia Artificial Generativa para producir contenido visual, aplicando t√©cnicas avanzadas de generaci√≥n como **GANs** (Redes Generativas Adversarias).\n",
    "\n",
    "La soluci√≥n desarrollada parte del uso del dataset **Fashion-MNIST** y emplea una arquitectura **DCGAN personalizada**, la cual fue entrenada, evaluada y comparada bajo diferentes configuraciones. El proyecto integra:\n",
    "\n",
    "- Experimentaci√≥n con m√∫ltiples configuraciones del modelo  \n",
    "- M√©tricas cuantitativas inspiradas en FID/IS  \n",
    "- Evaluaci√≥n cualitativa de im√°genes generadas  \n",
    "- An√°lisis comparativo de resultados  \n",
    "- Reflexiones √©ticas y t√©cnicas  \n",
    "- Implementaci√≥n de una interfaz interactiva para la generaci√≥n en tiempo real\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FzSe3cRF4f6p",
    "outputId": "f14bac20-e582-4b57-e134-dc701ed4c11d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "0VX2awqW79J9",
    "outputId": "60479b44-1502-44ec-eb15-e34ac36f2db3"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -------------------\n",
    "# Dataset Fashion-MNIST a 64x64\n",
    "# -------------------\n",
    "image_size = 64\n",
    "batch_size = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Ver algunas im√°genes reales\n",
    "images, _ = next(iter(train_loader))\n",
    "grid = utils.make_grid(images[:32], nrow=8, normalize=True)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(grid.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fashion-MNIST real (64x64)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AN√ÅLISIS EXPLORATORIO DE DATOS - FASHION-MNIST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================\n",
    "# 1. INFORMACI√ìN B√ÅSICA DEL DATASET\n",
    "# ============================\n",
    "\n",
    "print(f\"\\n Tama√±o del dataset: {len(train_dataset):,} im√°genes\")\n",
    "print(f\" Resoluci√≥n original: 28√ó28 p√≠xeles\")\n",
    "print(f\" Resoluci√≥n procesada: 64√ó64 p√≠xeles\")\n",
    "print(f\" Canales: 1 (escala de grises)\")\n",
    "print(f\" N√∫mero de clases: {len(class_names)}\")\n",
    "print(f\" Clases: {', '.join(class_names)}\")\n",
    "\n",
    "# ============================\n",
    "# 2. DISTRIBUCI√ìN DE CLASES\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISTRIBUCI√ìN DE CLASES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "labels = [label for _, label in train_dataset]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, 10))\n",
    "ax1.bar(class_names, counts, color=colors, edgecolor='black', linewidth=1.2)\n",
    "ax1.set_xlabel('Categor√≠a', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('N√∫mero de im√°genes', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribuci√≥n de clases en Fashion-MNIST', fontsize=14, fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "for i, (name, count) in enumerate(zip(class_names, counts)):\n",
    "    ax1.text(i, count + 100, f'{count:,}', ha='center', fontweight='bold')\n",
    "\n",
    "ax2.pie(counts, labels=class_names, autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "ax2.set_title('Proporci√≥n de clases', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_distribucion_clases.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset balanceado: Todas las clases tienen {counts[0]:,} im√°genes\")\n",
    "\n",
    "# ============================\n",
    "# 3. EJEMPLOS DE CADA CLASE\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EJEMPLOS DE CADA CATEGOR√çA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "\n",
    "    for img, label in train_dataset:\n",
    "        if label == i:\n",
    "            axes[i].imshow(img.squeeze(), cmap='gray')\n",
    "            axes[i].set_title(f'{class_name}', fontsize=11, fontweight='bold')\n",
    "            axes[i].axis('off')\n",
    "            break\n",
    "\n",
    "plt.suptitle('Ejemplos de cada categor√≠a en Fashion-MNIST',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_ejemplos_clases.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# 4. ESTAD√çSTICAS DE P√çXELES\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ESTAD√çSTICAS DE INTENSIDAD DE P√çXELES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "sample_images = []\n",
    "for i, (img, _) in enumerate(train_dataset):\n",
    "    if i >= 1000:\n",
    "        break\n",
    "    sample_images.append(img.numpy())\n",
    "\n",
    "sample_images = np.array(sample_images)\n",
    "\n",
    "print(f\"\\n M√≠nimo: {sample_images.min():.4f}\")\n",
    "print(f\" M√°ximo: {sample_images.max():.4f}\")\n",
    "print(f\" Media: {sample_images.mean():.4f}\")\n",
    "print(f\" Desviaci√≥n est√°ndar: {sample_images.std():.4f}\")\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "\n",
    "ax1.hist(sample_images.flatten(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('Intensidad de p√≠xel', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Frecuencia', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribuci√≥n de intensidades (normalizado)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "\n",
    "class_intensities = []\n",
    "for class_idx in range(10):\n",
    "    class_imgs = [img.numpy().flatten() for img, label in\n",
    "                  list(train_dataset)[:1000] if label == class_idx]\n",
    "    if class_imgs:\n",
    "        class_intensities.append(np.concatenate(class_imgs))\n",
    "\n",
    "ax2.boxplot(class_intensities, labels=class_names)\n",
    "ax2.set_xlabel('Categor√≠a', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Intensidad de p√≠xel', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Distribuci√≥n de intensidades por clase', fontsize=14, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_estadisticas_pixeles.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# 5. VARIABILIDAD INTRA-CLASE\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VARIABILIDAD DENTRO DE CADA CLASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for class_idx in range(10):\n",
    "\n",
    "    examples = []\n",
    "    for img, label in train_dataset:\n",
    "        if label == class_idx:\n",
    "            examples.append(img.squeeze().numpy())\n",
    "            if len(examples) == 9:\n",
    "                break\n",
    "\n",
    "\n",
    "    grid = np.concatenate([\n",
    "        np.concatenate(examples[0:3], axis=1),\n",
    "        np.concatenate(examples[3:6], axis=1),\n",
    "        np.concatenate(examples[6:9], axis=1)\n",
    "    ], axis=0)\n",
    "\n",
    "    axes[class_idx].imshow(grid, cmap='gray')\n",
    "    axes[class_idx].set_title(f'{class_names[class_idx]}', fontsize=11, fontweight='bold')\n",
    "    axes[class_idx].axis('off')\n",
    "\n",
    "plt.suptitle('Variabilidad intra-clase (9 ejemplos por categor√≠a)',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_variabilidad_clases.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============================\n",
    "# 6. RESUMEN VISUAL\n",
    "# ============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN DEL AN√ÅLISIS EXPLORATORIO\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Dataset Fashion-MNIST caracter√≠sticas:\n",
    "   - 60,000 im√°genes de entrenamiento\n",
    "   - 10 clases balanceadas (6,000 im√°genes c/u)\n",
    "   - Resoluci√≥n: 28√ó28 (escalado a 64√ó64)\n",
    "   - Escala de grises normalizada [-1, 1]\n",
    "\n",
    "Observaciones clave:\n",
    "   - Dataset perfectamente balanceado\n",
    "   - Buena variabilidad intra-clase\n",
    "   - Intensidades distribuidas uniformemente\n",
    "   - Adecuado para entrenamiento de GANs\n",
    "\n",
    "Archivos generados:\n",
    "   - eda_distribucion_clases.png\n",
    "   - eda_ejemplos_clases.png\n",
    "   - eda_estadisticas_pixeles.png\n",
    "   - eda_variabilidad_clases.png\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ijGqmIm63Ub"
   },
   "source": [
    "### Resultado del an√°lisis del dataset\n",
    "\n",
    "Se carg√≥ el dataset **Fashion-MNIST**, que contiene im√°genes de ropa en escala de grises.  \n",
    "Las im√°genes fueron redimensionadas a **64√ó64** p√≠xeles y normalizadas al rango \\([-1, 1]\\) para ser compatibles con la salida `Tanh` del generador.\n",
    "\n",
    "En la grilla superior se observa que:\n",
    "\n",
    "- Las prendas se visualizan correctamente (zapatos, camisas, abrigos, bolsos).  \n",
    "- El procesamiento previo es adecuado para la DCGAN.  \n",
    "- No hay distorsiones causadas por el `Resize`.\n",
    "\n",
    "Esto indica que los datos est√°n listos para su uso en el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNwRZz0G8IMY"
   },
   "outputs": [],
   "source": [
    "nz = 100   # tama√±o vector latente\n",
    "nc = 1     # canales (escala de grises)\n",
    "ngf = 32   # n√∫mero base de filtros en el generador (m√°s peque√±o que el t√≠pico 64)\n",
    "ndf = 32   # n√∫mero base de filtros en el discriminador\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkKqxY4q8NwA"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input: Z (nz x 1 x 1)\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),   # 4x4\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),  # 8x8\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),  # 16x16\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),      # 32x32\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),           # 64x64\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.main(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPGwlvDT7K0w"
   },
   "source": [
    "### Arquitectura del Generador\n",
    "\n",
    "El generador utiliza capas **ConvTranspose2d** para transformar un vector de ruido latente de dimensi√≥n 100 en una imagen de 64√ó64.\n",
    "\n",
    "Se utilizan:\n",
    "\n",
    "- Normalizaci√≥n `BatchNorm2d`  \n",
    "- Activaciones `LeakyReLU`  \n",
    "- Escalado final con `Tanh`  \n",
    "\n",
    "Esto permite:\n",
    "\n",
    "- Incrementar progresivamente la resoluci√≥n  \n",
    "- Reducir la amplificaci√≥n de ruido  \n",
    "- Mantener la estabilidad del entrenamiento  \n",
    "\n",
    "El dise√±o es eficiente y original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46ibLltj8S2B"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input: (nc, 64, 64)\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),    # 32x32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),  # 16x16\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),  # 8x8\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),  # 4x4\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),        # 1x1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        return out.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlE32d4P7drc"
   },
   "source": [
    "### Arquitectura del Discriminador\n",
    "\n",
    "El discriminador aprende a distinguir entre im√°genes reales y generadas empleando capas convolucionales `Conv2d`.\n",
    "\n",
    "Componentes importantes:\n",
    "\n",
    "- `LeakyReLU` para evitar apagado de gradientes  \n",
    "- `BatchNorm2d` para estabilizar el aprendizaje  \n",
    "- `Sigmoid` final para clasificar real/falso  \n",
    "\n",
    "Esta arquitectura permite capturar detalles locales t√≠picos de ropa (bordes, contornos y texturas simples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDRAmSBH8cFJ",
    "outputId": "dd629b65-92df-474e-b32c-83a4537af097"
   },
   "outputs": [],
   "source": [
    "netG = Generator().to(device)\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "print(netG)\n",
    "print(netD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tvl0Vlt78hGk"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIW6DmfQq6-J"
   },
   "outputs": [],
   "source": [
    "def create_models():\n",
    "    \"\"\"Crea un nuevo generador y discriminador en GPU/CPU.\"\"\"\n",
    "    netG = Generator().to(device)\n",
    "    netD = Discriminator().to(device)\n",
    "    return netG, netD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8xI2T1YvIh9"
   },
   "outputs": [],
   "source": [
    "nz = 100\n",
    "nc = 1\n",
    "ngf = 32\n",
    "ndf = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qNSRSdcP8mJd",
    "outputId": "21f778df-c420-4c26-8fcd-7a26cad9645a"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "os.makedirs(\"imagenes_generadas_dcgan\", exist_ok=True)\n",
    "\n",
    "# entrenar\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "\n",
    "        real_images = real_images.to(device)\n",
    "        b_size = real_images.size(0)\n",
    "\n",
    "        real_labels = torch.ones(b_size, 1, device=device)\n",
    "        fake_labels = torch.zeros(b_size, 1, device=device)\n",
    "\n",
    "        # ---------------------\n",
    "        # Entrenar Discriminador\n",
    "        # ---------------------\n",
    "        netD.zero_grad()\n",
    "\n",
    "        output_real = netD(real_images)\n",
    "        loss_real = criterion(output_real, real_labels)\n",
    "\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake_images = netG(noise)\n",
    "        output_fake = netD(fake_images.detach())\n",
    "        loss_fake = criterion(output_fake, fake_labels)\n",
    "\n",
    "        lossD = loss_real + loss_fake\n",
    "        lossD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # Entrenar Generador\n",
    "        # ---------------------\n",
    "        netG.zero_grad()\n",
    "\n",
    "        output_fake_for_G = netD(fake_images)\n",
    "        lossG = criterion(output_fake_for_G, real_labels)\n",
    "        lossG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "    G_losses.append(lossG.item())\n",
    "    D_losses.append(lossD.item())\n",
    "\n",
    "    # im√°genes de monitoreo\n",
    "    with torch.no_grad():\n",
    "        fake = netG(fixed_noise).detach().cpu()\n",
    "    grid = utils.make_grid(fake, nrow=8, normalize=True)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Im√°genes generadas (DCGAN) ‚Äî √âpoca {epoch+1}\")\n",
    "    plt.show()\n",
    "\n",
    "    utils.save_image(fake, f\"imagenes_generadas_dcgan/epoch_{epoch+1:03d}.png\",\n",
    "                     nrow=8, normalize=True)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]  Loss_D: {lossD.item():.4f}  Loss_G: {lossG.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnFGDNre7709"
   },
   "source": [
    "### Entrenamiento de la DCGAN\n",
    "\n",
    "Durante el entrenamiento:\n",
    "\n",
    "- El discriminador aprende primero a distinguir im√°genes reales de las generadas.  \n",
    "- El generador corrige sus pesos gradualmente para producir im√°genes m√°s realistas.  \n",
    "- Se utiliza `Adam` con tasa de aprendizaje `0.0002`, un valor t√≠pico para GANs.\n",
    "\n",
    "Los primeros resultados suelen parecer ruido, pero a partir de las √©pocas 7‚Äì10 se observa:\n",
    "\n",
    "- Mayor definici√≥n de bordes  \n",
    "- Formas reconocibles  \n",
    "- Menos artefactos del generador  \n",
    "\n",
    "Las im√°genes son consistentes con las caracter√≠sticas del dataset de prendas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nx9JrS5X8tRg"
   },
   "outputs": [],
   "source": [
    "def train_experiment(\n",
    "    experiment_name,\n",
    "    num_epochs=10,\n",
    "    lr_G=0.0002,\n",
    "    lr_D=0.0002,\n",
    "    max_batches=200\n",
    "):\n",
    "    print(f\"\\nüî¨ Iniciando experimento: {experiment_name}\")\n",
    "\n",
    "    netG = Generator().to(device)\n",
    "    netD = Discriminator().to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr_D, betas=(0.5, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=(0.5, 0.999))\n",
    "\n",
    "    fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "\n",
    "    outfolder = f\"imagenes_{experiment_name}\"\n",
    "    os.makedirs(outfolder, exist_ok=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_images, _) in enumerate(train_loader):\n",
    "\n",
    "            if i >= max_batches:  # acelera entrenamiento\n",
    "                break\n",
    "\n",
    "            real_images = real_images.to(device)\n",
    "            b_size = real_images.size(0)\n",
    "\n",
    "            real_labels = torch.ones(b_size, 1, device=device)\n",
    "            fake_labels = torch.zeros(b_size, 1, device=device)\n",
    "\n",
    "            # ---- Entrenar D ----\n",
    "            netD.zero_grad()\n",
    "            output_real = netD(real_images)\n",
    "            loss_real = criterion(output_real, real_labels)\n",
    "\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            fake_images = netG(noise)\n",
    "            output_fake = netD(fake_images.detach())\n",
    "            loss_fake = criterion(output_fake, fake_labels)\n",
    "\n",
    "            lossD = loss_real + loss_fake\n",
    "            lossD.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            # ---- Entrenar G ----\n",
    "            netG.zero_grad()\n",
    "            output_fake_for_G = netD(fake_images)\n",
    "            lossG = criterion(output_fake_for_G, real_labels)\n",
    "            lossG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "        G_losses.append(lossG.item())\n",
    "        D_losses.append(lossD.item())\n",
    "\n",
    "        # Guardar im√°genes\n",
    "        with torch.no_grad():\n",
    "            fake = netG(fixed_noise).detach().cpu()\n",
    "\n",
    "        utils.save_image(\n",
    "            fake, f\"{outfolder}/{experiment_name}_epoch_{epoch+1:03d}.png\",\n",
    "            normalize=True, nrow=8\n",
    "        )\n",
    "\n",
    "        # Mostrar la imagen generada en Colab\n",
    "        grid = utils.make_grid(fake, nrow=8, normalize=True)\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(grid.permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Im√°genes generadas ‚Äì {experiment_name} ‚Äì Epoch {epoch+1}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}  Loss_D: {lossD.item():.4f}  Loss_G: {lossG.item():.4f}\")\n",
    "\n",
    "    # Guardar modelo\n",
    "    os.makedirs(\"modelos\", exist_ok=True)\n",
    "    torch.save(netG.state_dict(), f\"modelos/{experiment_name}.pth\")\n",
    "\n",
    "    return netG, netD, G_losses, D_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "232G0MDq8zn8",
    "outputId": "720f1afa-022c-436d-c8ad-a9c2ad47b56f"
   },
   "outputs": [],
   "source": [
    "exp1 = train_experiment(\n",
    "    experiment_name=\"exp1_baseline\",\n",
    "    num_epochs=10,\n",
    "    lr_G=0.0002,\n",
    "    lr_D=0.0002,\n",
    "    max_batches=200\n",
    ")\n",
    "\n",
    "exp2 = train_experiment(\n",
    "    experiment_name=\"exp2_mas_epocas\",\n",
    "    num_epochs=20,\n",
    "    lr_G=0.0002,\n",
    "    lr_D=0.0002,\n",
    "    max_batches=200\n",
    ")\n",
    "\n",
    "exp3 = train_experiment(\n",
    "    experiment_name=\"exp3_lrD_bajo\",\n",
    "    num_epochs=20,\n",
    "    lr_G=0.0002,\n",
    "    lr_D=0.0001,\n",
    "    max_batches=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aOCMHpOB-VU"
   },
   "source": [
    "## An√°lisis unificado de los tres experimentos\n",
    "\n",
    "Se realizaron tres experimentos modificando el n√∫mero de √©pocas y la tasa de aprendizaje del discriminador con el fin de evaluar el impacto de estos hiperpar√°metros en la calidad de las im√°genes generadas por la DCGAN.\n",
    "\n",
    "En t√©rminos generales, los tres experimentos muestran un patr√≥n de evoluci√≥n similar: en las primeras √©pocas el generador produce √∫nicamente ruido, luego aparecen formas borrosas, y progresivamente surgen siluetas de prendas con mayor claridad y consistencia. Sin embargo, cada experimento presenta comportamientos particulares que permiten compararlos.\n",
    "\n",
    "### Aparici√≥n de formas y estabilidad\n",
    "En el **Experimento 1 (baseline, 10 √©pocas)** las prendas comienzan a ser reconocibles alrededor de la mitad del entrenamiento. Se observan camisetas, vestidos, zapatos y abrigos con cierto grado de distorsi√≥n, pero la estructura general es coherente. Este experimento sirve como referencia para evaluar mejoras posteriores.\n",
    "\n",
    "En el **Experimento 2 (20 √©pocas)** la calidad visual mejora notablemente. Las im√°genes finales son m√°s n√≠tidas, con bordes mejor definidos y mayor estabilidad. La red tiene m√°s tiempo para refinar detalles, lo cual se refleja en contornos m√°s limpios y menos ruido. Este experimento obtiene las muestras m√°s claras y coherentes.\n",
    "\n",
    "En el **Experimento 3 (lr_D m√°s bajo)**, al disminuir la tasa de aprendizaje del discriminador, el generador tiene m√°s libertad para explorar la distribuci√≥n. Esto se traduce en una mayor **diversidad** de prendas generadas: se observan m√°s clases diferentes y menos repetici√≥n de patrones. Aunque algunas im√°genes son algo m√°s borrosas que en el Experimento 2, el aumento en variedad es evidente.\n",
    "\n",
    "### Conclusi√≥n general\n",
    "Los resultados permiten concluir que:\n",
    "\n",
    "- **M√°s √©pocas** mejoran el realismo (Experimento 2).  \n",
    "- **Un discriminador m√°s suave** aumenta la diversidad (Experimento 3).  \n",
    "- El experimento baseline es funcional, pero queda superado por las configuraciones alternativas.  \n",
    "\n",
    "En resumen, el entrenamiento prolongado produce las mejores im√°genes desde el punto de vista visual, mientras que disminuir la fuerza del discriminador fomenta la variedad de clases generadas. Cada configuraci√≥n ofrece un balance distinto entre nitidez y diversidad, lo cual es √∫til seg√∫n el objetivo final de la aplicaci√≥n (mayor realismo o mayor variabilidad en las prendas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPwI8uEyDHZj",
    "outputId": "3358963e-f8d6-4e8c-e8e2-1b0f18a10559"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*16*16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "clf = SimpleCNN().to(device)\n",
    "opt_clf = optim.Adam(clf.parameters(), lr=0.001)\n",
    "criterion_clf = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):\n",
    "    total, correct, loss_sum = 0, 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        opt_clf.zero_grad()\n",
    "        out = clf(imgs)\n",
    "        loss = criterion_clf(out, labels)\n",
    "        loss.backward()\n",
    "        opt_clf.step()\n",
    "        loss_sum += loss.item()\n",
    "        _, preds = out.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss_sum/len(train_loader):.4f} | Acc: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xI_TbOmIDw_4"
   },
   "outputs": [],
   "source": [
    "def evaluar_generador(path, samples=500, batch_size=64):\n",
    "    netG = Generator().to(device)\n",
    "    netG.load_state_dict(torch.load(path, map_location=device))\n",
    "    netG.eval()\n",
    "    clf.eval()\n",
    "\n",
    "    probs_list = []\n",
    "    preds_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(samples // batch_size):\n",
    "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            out = clf(fake)\n",
    "            probs = torch.softmax(out, dim=1)\n",
    "            maxp, preds = probs.max(1)\n",
    "            probs_list.append(maxp.cpu())\n",
    "            preds_list.append(preds.cpu())\n",
    "\n",
    "    probs = torch.cat(probs_list)\n",
    "    preds = torch.cat(preds_list)\n",
    "\n",
    "    realism = probs.mean().item()\n",
    "    diversity = len(preds.unique()) / 10\n",
    "\n",
    "    p_y = probs.mean()\n",
    "    is_proxy = (probs * (probs.log() - p_y.log())).exp().mean().item()\n",
    "\n",
    "    return realism, diversity, is_proxy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "7u91wl8JD2Fs",
    "outputId": "6301ceec-f5a2-49e1-a57b-b221b3155a71"
   },
   "outputs": [],
   "source": [
    "r1, d1, is1 = evaluar_generador(\"modelos/exp1_baseline.pth\")\n",
    "r2, d2, is2 = evaluar_generador(\"modelos/exp2_mas_epocas.pth\")\n",
    "r3, d3, is3 = evaluar_generador(\"modelos/exp3_lrD_bajo.pth\")\n",
    "\n",
    "import pandas as pd\n",
    "resultados = pd.DataFrame({\n",
    "    \"Experimento\": [\"Exp1 - baseline\", \"Exp2 - m√°s √©pocas\", \"Exp3 - lrD bajo\"],\n",
    "    \"Realism Score\": [r1, r2, r3],\n",
    "    \"Diversity Score\": [d1, d2, d3],\n",
    "    \"IS Proxy\": [is1, is2, is3]\n",
    "})\n",
    "\n",
    "resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgoD4KAMEbCk"
   },
   "source": [
    "### Interpretaci√≥n de las m√©tricas\n",
    "\n",
    "- **Realism Score:** indica qu√© tan claras son las im√°genes para el clasificador.  \n",
    "- **Diversity Score:** mide cu√°ntas clases distintas genera la GAN (ideal: cercano a 1).  \n",
    "- **IS Proxy:** balance entre calidad y variedad.\n",
    "\n",
    "Seg√∫n los resultados:\n",
    "- El **Experimento 2** mejora el realismo.  \n",
    "- El **Experimento 3** maximiza la diversidad.  \n",
    "- El **Experimento 1** sirve como referencia y muestra los valores m√°s bajos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "WZ77AodAFFVt",
    "outputId": "ebd66259-2192-44f8-e083-ec404f2664b8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(resultados[\"Experimento\"], resultados[\"Realism Score\"], marker='o', label=\"Realism\")\n",
    "plt.plot(resultados[\"Experimento\"], resultados[\"Diversity Score\"], marker='o', label=\"Diversity\")\n",
    "plt.plot(resultados[\"Experimento\"], resultados[\"IS Proxy\"], marker='o', label=\"IS Proxy\")\n",
    "\n",
    "plt.title(\"Comparaci√≥n de m√©tricas por experimento\")\n",
    "plt.ylabel(\"Valor de la m√©trica\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj_gBcq4FWa4"
   },
   "source": [
    "### Interpretaci√≥n de la comparaci√≥n de m√©tricas entre experimentos\n",
    "\n",
    "En la figura se comparan tres m√©tricas calculadas para cada experimento: **Realism Score**, **Diversity Score** y **IS Proxy**. Los valores muestran un patr√≥n consistente entre los modelos, con peque√±as variaciones que permiten identificar fortalezas espec√≠ficas de cada configuraci√≥n.\n",
    "\n",
    "- **Realism Score:**  \n",
    "  Presenta ligeras diferencias entre los tres experimentos.  \n",
    "  El *Experimento 2* obtiene el valor m√°s alto, lo cual indica que entrenar m√°s √©pocas permite que el generador produzca im√°genes ligeramente m√°s claras y reconocibles.  \n",
    "  El *Experimento 3* tambi√©n logra un realismo adecuado, aunque un poco menor debido al discriminador m√°s suave.\n",
    "\n",
    "- **Diversity Score:**  \n",
    "  Los tres experimentos alcanzan un puntaje cercano a **1.0**, lo que indica que en todos los casos el generador es capaz de producir im√°genes correspondientes a las distintas clases de Fashion-MNIST (zapatos, camisetas, vestidos, pantalones, etc.).  \n",
    "  Esto confirma que **no hay colapso de modo** y que la GAN genera variedad de prendas.\n",
    "\n",
    "- **IS Proxy:**  \n",
    "  Esta m√©trica eval√∫a el balance entre realismo y diversidad.  \n",
    "  En este caso, las diferencias entre los experimentos son m√≠nimas, pero nuevamente el *Experimento 2* muestra un rendimiento ligeramente superior.  \n",
    "  Esto coincide con la evaluaci√≥n cualitativa, donde se vio mayor nitidez y definici√≥n de las prendas.\n",
    "\n",
    "### Conclusi√≥n general de las m√©tricas\n",
    "\n",
    "Las m√©tricas cuantitativas confirman la evaluaci√≥n visual:\n",
    "\n",
    "- El **Experimento 2** (m√°s √©pocas) es el que logra mejor equilibrio entre claridad y estabilidad.  \n",
    "- El **Experimento 3** mantiene buena diversidad sin perder calidad.  \n",
    "- El **Experimento 1** sirve como baseline y es ligeramente inferior, pero consistente.\n",
    "\n",
    "En conjunto, los resultados muestran que los ajustes de hiperpar√°metros influyen en peque√±a medida en el desempe√±o global, pero permiten optimizar realismo o diversidad seg√∫n el objetivo final de la aplicaci√≥n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgXUm9ymEiyu"
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "- La DCGAN personalizada entrenada sobre Fashion-MNIST logra generar prendas reconocibles.  \n",
    "- Los tres experimentos muestran que la calidad visual depende fuertemente de los hiperpar√°metros.  \n",
    "- El mejor compromiso entre realismo y diversidad se obtiene en los experimentos 2 y 3.  \n",
    "- Las m√©tricas cuantitativas confirman la evaluaci√≥n visual.  \n",
    "- Como l√≠neas de mejora futura: aumentar la resoluci√≥n, usar modelos de difusi√≥n y expandir el dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc-3STMMuEDf"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"juliana.pena@est.iudigital.edu.co\"\n",
    "!git config --global user.name \"julimariadev\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0YGVaq7usBT",
    "outputId": "845789ca-fdcc-45a1-e57f-fbcf0dab5c2f"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/julimariadev/IA_GENERATIVA_DEEPLEARNING.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0sEhaixwSOQ",
    "outputId": "48880d93-b9ca-45cb-c536-97926a0ec42f"
   },
   "outputs": [],
   "source": [
    "!ls /content\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
